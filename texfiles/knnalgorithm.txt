def construct_tree(X, current_height, max_height):
  """The function constructs a tree/sub-tree on points X.

  current_height: represents the height of the current tree to
    the root of the decision tree.
  max_height: the max height of the tree that should be constructed.

  The current_height and max_height only exists to make the algorithm efficient
  as we assume that no anomalies exist at depth >= max_height.
  """
  if current_height >= max_height:
    # here we are sure that no anomalies exist hence we
    # directly construct the external node.
    return new_external_node(X)

  # pick any attribute at random.
  attribute = get_random_attribute(X)

  # for set of inputs X, for the tree we get a random value
  # for the chosen attribute. preferably around the median.
  split_value = get_random_value(max_value, min_value)

  # split X instances based on `split_values` into Xl and Xr
  Xl = filter(X, lambda x: X[attribute] < split_value)
  Xr = filter(X, lambda x: X[attribute] >= split_value)

  # build an internal node with its left subtree created from Xl
  # and right subtree created from Xr, recursively.
  return new_internal_node(
    left=construct_tree(Xl, current_height + 1, max_height),
    right=construct_tree(Xr, current_height + 1, max_height),
    split_attribute=attribute,
    split_value=split_value,
  )

https://www.codementor.io/@arpitbhayani/isolation-forest-algorithm-for-anomaly-detection-133euqilki

https://algobeans.com/2016/09/14/k-nearest-neighbors-anomaly-detection-tutorial/

#!/usr/bin/python
import csv, numpy
from sklearn.neighbors import KNeighborsClassifier
from sklearn.preprocessing import MinMaxScaler

# To save time and avoid attribute reconstruction, we have prebuilt training and testing files 
# where the attributes are presented under CSV format.
# We just need to convert these files into matrices so they can be used directly as input 
# of the machine learning algorithms.

def csv_attributes_to_matrix(csv_file):
    with open(csv_file, 'r') as data:
        rows = csv.reader(data, delimiter=',', quoting=csv.QUOTE_NONNUMERIC)
        return MinMaxScaler().fit_transform(numpy.array(list(rows)))

def csv_labels_to_matrix(csv_file):
    with open(csv_file, 'r') as data:
        rows = csv.reader(data, delimiter=',')
        return [(int(row[0])) for row in rows]

# Create vectors of normal labels for training => all-1-vector
def create_normal_vectors(MATRIX_NORM_length):
    y = list()
    for l in range(MATRIX_NORM_length):
        y.append(1)
    return numpy.array(y)

# Test of KNeighborsClassifier for anomaly detection
def kNN_test(MATRIX_NORM, MATRIX_ANOM, real_labels):
    Y = create_normal_vectors(len(MATRIX_NORM))

    # Parameter grid search    
    for n_neighbors in [1, 2, 3, 5, 10]:
        for weights in ["uniform", "distance"]:
            for algo in ["ball_tree", "kd_tree", "brute"]:
                for p in [1, 5, 10]:
                    for leaf_size in [1, 5, 10] if algo in ["ball_tree", "kd_tree"] else [None]:
                        trained_model = KNeighborsClassifier(n_neighbors, weights, algo, leaf_size, p)
                        trained_model.fit(MATRIX_NORM, Y)
                        predicted_labels = trained_model.predict(MATRIX_ANOM)
                        # Predicted labels are always all set to 1, why ?
                        print (n_neighbors, weights, algo, p, leaf_size), "\n", predicted_labels

# Normal (training) and anomalous (testing) input csv files:
MATRIX_NORM = csv_attributes_to_matrix("training_samples.csv")
MATRIX_ANOM = csv_attributes_to_matrix("testing_samples.csv")
real_labels = csv_labels_to_matrix("testing_labels.csv")

# Launch test
kNN_test(MATRIX_NORM, MATRIX_ANOM, real_labels)
https://stackoverflow.com/questions/21416572/anomaly-detection-using-k-nearest-neighbors